#####################################################################################
# Copyright 2019 National Technology & Engineering Solutions of Sandia, LLC (NTESS).
# Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains
# certain rights in this software.
#####################################################################################
# NOTICE:
# For five (5) years from 10/21/2019 the United States Government is granted for
# itself and others acting on its behalf a paid-up, nonexclusive, irrevocable worldwide
# license in this data to reproduce, prepare derivative works, and perform publicly and
# display publicly, by or on behalf of the Government. There is provision for the
# possible extension of the term of this license. Subsequent to that period or any
# extension granted, the United States Government is granted for itself and others
# acting on its behalf a paid-up, nonexclusive, irrevocable worldwide license in this
# data to reproduce, prepare derivative works, distribute copies to the public,
# perform publicly and display publicly, and to permit others to do so. The specific
# term of the license can be identified by inquiry made to National Technology and
# Engineering Solutions of Sandia, LLC or DOE. NEITHER THE UNITED STATES GOVERNMENT,
# NOR THE UNITED STATES DEPARTMENT OF ENERGY, NOR NATIONAL TECHNOLOGY AND ENGINEERING
# SOLUTIONS OF SANDIA, LLC, NOR ANY OF THEIR EMPLOYEES, MAKES ANY WARRANTY, EXPRESS OR
# IMPLIED, OR ASSUMES ANY LEGAL RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR
# USEFULNESS OF ANY INFORMATION, APPARATUS, PRODUCT, OR PROCESS DISCLOSED, OR REPRESENTS
# THAT ITS USE WOULD NOT INFRINGE PRIVATELY OWNED RIGHTS. Any licensee of this software
# has the obligation and responsibility to abide by the applicable export control laws,
# regulations, and general prohibitions relating to the export of technical data.
# Failure to obtain an export control license or other authority from the Government
# may result in criminal liability under U.S. laws.
# (End of Notice)
####################################################################################

__all__ = ["DCOffSetTimesMetric"]

import numpy as np
from pycheron.util.logger import Logger
from pycheron.util.masks import samples2time


def DCOffSetTimesMetric(
    st,
    windowSecs=1800,
    incrementSecs=None,
    threshold=0.9,
    generateMasks=False,
    masksByTime=True,
    logger=None,
    database=None,
):
    """Metric to determine times of DC offsets (or where a shift in the signal mean is detected)

    :param st: obspy stream object
    :type st: obspy.core.stream.Stream
    :param windowSecs: Chunk size (secs) used in DCOffset calculations
    :type windowSecs: int
    :param incrementSecs: Increment (secs) for starttime of sequential chunks (DEFAULT = windowSecs/2)
    :type incrementSecs: int
    :param threshold: Threshold used in the detection metric.
    :type threshold: float
    :param generateMasks: If True, a boolean mask will be created for dc offset.
    :type generateMasks: bool
    :param masksByTime: Boolean to determine whether masks are generated by time. If True, masks will be generated with a start/end time, if false, they will be generated as boolean array.
    :type masksByTime: bool
    :param logger: (logger object) - If using a logger, (you must create one using the util.logger class)
    :type logger: pycheron.util.logger.Logger
    :param database: database object
    :type database: pycheron.db.sqllite_db.Database

    :return: list of dictionaries containing the following keys and types:

                * snclq (`str`)
                * start_time (`str`)
                * end_time (`str`)
                * dc_offset_times (`list`)
                * masks (`list` or `numpy.ndarray`)
                * metric_name (`str`)

    :rtype: dict

    **Metric asserts:** If difference in means between sequential chunks of seismic signal is greater than the typical
    std deviation of a chunk then this marks a DC offset shift

    **Algorithm steps:**

    #. Break up signal into windowSecs chunks spaced incrementSecs apart
    #. For each chunk calculate: signal mean, signal std deviation
    #. Resulting mean and std dev arrays are of length 47 for 24 hours of signal
    #. Metric = abs(lagged difference of chunk means)/mean(chunk std devs)
    #. DC offset = times when metric > threshold

    **Examples**

    .. code-block:: python

        # Read in data
        data = 'test/test_data/7A_CABN_ALL.988887.tar.mseed'
        st = obspy.read(data)
        # Calculate DCOffSetTimesmetric
        d = DCOffSetTimesMetric(st,1800,None,0.9, True)
        # Convert BHE stream to trace object
        tr = st[1]
        print 'dcOffsetTimes for BHE channel:', d[1]
        >>> 'dcOffsetTimes for BHE channel:' {'metric_name': 'DCOffsetTimesMetric', 'dc_offset_times': [UTCDateTime(2013, 11, 1, 0, 45, 0, 25000), UTCDateTime(2013, 11, 1, 1, 15, 0, 25000), UTCDateTime(2013, 11, 1, 9, 30, 0, 25000), UTCDateTime(2013, 11, 1, 9, 45, 0, 25000), UTCDateTime(2013, 11, 1, 10, 0, 0, 25000), UTCDateTime(2013, 11, 1, 10, 15, 0, 25000), UTCDateTime(2013, 11, 1, 10, 30, 0, 25000), UTCDateTime(2013, 11, 1, 10, 45, 0, 25000), UTCDateTime(2013, 11, 1, 15, 45, 0, 25000), UTCDateTime(2013, 11, 1, 16, 15, 0, 25000)], 'snclq': u'7A.CABN..BHE.M', 'start_time': UTCDateTime(2013, 11, 1, 0, 0), 'masks': array([0, 0, 1, ..., 0, 0, 0]), 'end_time': UTCDateTime(2013, 11, 1, 23, 59, 59, 975000)}

    .. code-block:: python

        # Plotting
        de = []
        for i in d[1]['dc_offset_times']:
            de.append(timedelta.total_seconds(i.datetime-tr.stats.starttime.datetime))
        plt.plot(tr.times(),tr.data)
        for i in de:
            plt.axvline(x=i,color='black')
        plt.xlabel('Seconds from %s' %str(tr.stats.starttime))
        plt.xlim([min(tr.times()),max(tr.times())])
        plt.ylabel('Amplitude')
        plt.title('%s WindowSecs 1800, incrementSecs 900, threshold 0.9'%str(tr.id))
        plt.show()

    .. image:: _static/DCOffSetTimesMetric.png

    """
    if logger == None:
        logger = Logger(None)

    # Initialize incrementSecs variable
    if incrementSecs is None:
        incrementSecs = windowSecs / 2

    # Initialize d, a list of dictionaries to fill with metric values
    d = []

    # Merge stream objects with same IDs, if gaps present, fill with '999999', then remove '999999' values from trace
    # Make copy of stream object to retain original stream, otherwise will be permanently changed
    stC = st.copy()
    stM = stC.merge(fill_value=999999)
    for trace in stM:
        tr = trace
        tr.data = tr.data[tr.data != 999999]

    # Loop through traces and get trace information, chunk information, then calculate mean and std dev for chunks
    for trace in stM:
        # Get trace information: snclq, starttime, endtime for each segment
        tr = trace
        fs = tr.stats.sampling_rate

        snclq = tr.get_id()

        starttime = tr.stats.starttime
        endtime = tr.stats.endtime

        # Calculate chunk information
        windowSamples = windowSecs * tr.stats.sampling_rate
        incrementSamples = incrementSecs * tr.stats.sampling_rate
        outLength = int(len(tr) / incrementSamples)

        # Initialize mean, stddev, and indice lists
        means = []
        sds = []
        indices = []

        # Loop through all of the chunks and calculate mean and std dev
        for i in range(outLength):
            lo = int(i * incrementSamples + 1)
            hi = int(lo + windowSamples)
            means.append(np.mean(tr.data[lo:hi]))
            sds.append(np.std(tr.data[lo:hi]))
            indices.append(int(lo))

        # Calculate the difference between sequential means
        # Determine indices where the value is greater than the provided threshold
        # Use those indices to determine dcOffset times and append to dcOffsetTimes list

        # if metric_store is None:
        #     mean_sds = np.mean(sds)
        # else:
        #     pass
        #     # update metric store with np.mean(sds)
        #     # get mean_sds from metric_store

        metric = abs(np.diff(means, n=1, axis=0)) / np.mean(sds)
        jumps = [index for index, value in enumerate(metric) if value > threshold]
        dcOffsetTimes = []
        for i in jumps:
            times = starttime + indices[i + 1] / tr.stats.sampling_rate
            dcOffsetTimes.append(times.isoformat())

        # creating masks
        masks = None
        if generateMasks == True and jumps:
            if masksByTime:
                masks = samples2time(np.asarray(jumps), fs, starttime)
            else:
                masks = np.zeros(len(tr.data), dtype=int)
                masks[jumps] = 1

        # Create metrics list to append to from each stream object
        metrics = {
            "snclq": snclq,
            "start_time": starttime.isoformat(),
            "end_time": endtime.isoformat(),
            "dc_offset_times": dcOffsetTimes,
            "masks": masks,
            "metric_name": "DCOffsetTimesMetric",
        }

        d.append(metrics)

    if database is not None:
        database.insert_metric(d)
    return d
